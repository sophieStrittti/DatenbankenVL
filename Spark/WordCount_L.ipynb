{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"word count\") \\\n",
    "      .getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frank=spark.read.text(\"frankenstein.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea31f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# und siehe da, es ist ein Dataframe, er hat 1 Spalte, und sie hat den Datentyp \"String\"\n",
    "frank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# das Schema explizit anzeigen\n",
    "frank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# und wenn wir auf das Schema direkt zugreifen wollen, dann gibt es alternativ die dtypes\n",
    "print(frank.dtypes)\n",
    "print(f\"Ist das Element == 'string': { frank.dtypes[0][1] == 'string'}\")\n",
    "# Achtung, was passiert hier ... potentieller Denkfehler!\n",
    "print(f'der Spaltenname ist {frank.dtypes[0][0]} und ist der Typ String?: { isinstance(frank.dtypes[0][1], str)}')\n",
    "print(f'der Spaltenname ist {frank.dtypes[0][0]} und ist der Typ Int?: { isinstance(frank.dtypes[0][1], int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617948d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeige die ersten 30 Zeilen komplett (nicht abgeschnitten (truncated))\n",
    "frank.show(30,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac8d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jetzt die Zeilen in einzelne Worte splitten\n",
    "from pyspark.sql.functions import split\n",
    "# select selektiert eine oder mehrere Spalten, hier eben nur die 1 Spalte \"value\"\n",
    "# alias gibt dem Ergebnis (der selektierten Spalte) einen Namen (sonst waere es eher unhandlich)\n",
    "# die Funktion split nimmt uebrigens eine REGEXP ... hier ist sie halt super-einfach> ein space\n",
    "lines = frank.select(split(frank.value, \" \").alias(\"Zeile\"))\n",
    "lines.show(10, truncate=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a941446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# das Schema: das Split hat ein Array generiert\n",
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eine Alternative zu ALIAS ist \"withColumnRenamed\" - die nimmt einen existierenden Dataframe und \n",
    "# erstellt einen neuen, der eben nun eine umbenannte Spalte hat\n",
    "linesWRC = lines.withColumnRenamed(\"Zeile\", \"ZeileRenamed\")\n",
    "linesWRC.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a187f16",
   "metadata": {},
   "source": [
    "das gefaellt mir noch nicht so richtig, z.B. das Komma nach Frankenstein, oder die leere Zeile, \n",
    "oder die Gross- und Kleinschreibung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frank.show(100, truncate=False) # nur zum ueberpruefen ob die Regex unten tut, was sie soll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75fd4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Schritt 1: regexp verbessern, so dass das Zeichensetzung, Zahlen etc wegfällt\n",
    "lines = frank.select(split(frank.value, \"[^a-zA-Z]\").alias(\"Zeile\"))\n",
    "lines.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalte selektieren geht auf viele Arten:\n",
    "lines.select(lines.Zeile).show()\n",
    "lines.select(\"Zeile\").show()\n",
    "from pyspark.sql.functions import col\n",
    "lines.select(col(\"Zeile\")).show()\n",
    "# wohingegen das hier nicht geht: das ist naemlich eine Column, kein Dataframe!\n",
    "# lines[\"Zeile\"].show()\n",
    "# aber diese Spalte nutzen um dann eine Spalte im Dataframe auszuwaehlen, das geht:\n",
    "lines.select(lines[\"Zeile\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2eca5",
   "metadata": {},
   "source": [
    "# Exploding list of words into ROWS (nicht COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "words = lines.select(explode(col(\"Zeile\")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f824d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huebsch ist das immer noch nicht. \n",
    "# Schritt 1: alles lower-case bitte\n",
    "from pyspark.sql.functions import lower\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "words_lower.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weg mit den kurzen Zeilen bitte\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "# minimum laenge 2 Zeichen ausser dem Wort \"a\" und \"I\"\n",
    "words_clean = words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]{2,}|a|i\", 0).alias(\"echtesWort\"))\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1366fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weg mit den leeren Zeilen bitte\n",
    "proper_words = words_clean.filter(col(\"echtesWort\") != \"\")\n",
    "proper_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442efa1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nur zur Illustration, filter kann Negation: ~\n",
    "proper_words_negation = words_clean.filter(col(\"echtesWort\") != \"\")\n",
    "proper_words_negation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5b9a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vergleiche mit WHERE / gibt es da einen Unterschied?\n",
    "proper_wordsWHERE = words_clean.where(col(\"echtesWort\") != \"\")\n",
    "proper_wordsWHERE.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a78c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up Aufgabe zu 1b: aendere den Code von vorhin so dass\n",
    "#-nur Woerter mit der Mindestlaenge von 3 Zeichen beibehalten werden diesmal mit der Funktion \"length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antwort\n",
    "from pyspark.sql.functions import length\n",
    "min3Zeichen = words_nonull.where(length(col(\"word\")) > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0192508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe: \n",
    "# was passiert hier - und was passiert, wenn man es negiert?\n",
    "proper_words_any = words_clean.filter(col(\"echtesWort\") != \"any*\")\n",
    "proper_words_any.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbfb1d",
   "metadata": {},
   "source": [
    "# Die große Frage: in welcher Reihenfolge filtern? Wie crazy muss die Regex sein?\n",
    "Antwort: Lesbarkeit geht vor. Spark evaluiert lazy, und kann \"hintendran\" sehr viel optimieren. Ist wahrscheinlich \n",
    "schlauer als wir;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794c1f5",
   "metadata": {},
   "source": [
    "# Aufgabe 1: erstelle einen Block \"sauberen\" Code, der Schritt fuer Schritt alle Einzelschritt ausfuehrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7f83e1a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Antwort / die einen Fehler enthaelt, der aber gar nicht wie ein Fehler aussieht\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "frank = spark.read.text(\"frankenstein.txt\")\n",
    "\n",
    "lines = frank.select(split(frank.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "\n",
    "words_clean = words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]{2+}|a|i\", 0).alias(\"word\"))\n",
    "\n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e3aaa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| word|\n",
      "+-----+\n",
      "|hello|\n",
      "|world|\n",
      "| this|\n",
      "| boat|\n",
      "|  has|\n",
      "|     |\n",
      "|   no|\n",
      "| oars|\n",
      "|world|\n",
      "|  has|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Frage: was passiert, wenn man das hier ausfuehrt?\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d913ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antwort: es gibt einen Fehler. Ja he, das ging doch oben einwandfrei durch!\n",
    "# des Raetsels Loesung: im Regex ist ein Fehler: das {2+} geht nicht, sonder man muss {2,} benutzen.\n",
    "# da es aber lazy evaluation ist, finden wir das erst heraus, wenn das show aufgerufen wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c95f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 1b: aendere den Code von eben so dass\n",
    "- das Wort \"is\" aus dem gesamten Text entfernt wird\n",
    "- nur Woerter mit der Mindestlaenge von 3 Zeichen beibehalten werden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antwort:\n",
    "# letzte Zeile: words_nonull = words_clean.where(col(\"word\") != \"is\")\n",
    "#words_clean = words_lower.select(\n",
    "#    regexp_extract(col(\"word_lower\"), \"[a-z]{3+}\", 0).alias(\"word\")\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70a763",
   "metadata": {},
   "source": [
    "# Aufgabe 2: finde programmatisch heraus, wie viele Spalten KEINE Strings sind:\n",
    "datenA2 = spark.createDataFrame([[\"test\", \"noch ein test\", 10_000_000_000]], [\"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datenA2 = spark.createDataFrame([[\"test\", \"noch ein test\", 10_000_000_000]], [\"1\", \"2\", \"3\"])\n",
    "datenA2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e980ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([x for x, y in datenA2.dtypes if y != \"string\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oder fuer die Nicht-Python Programmierer\n",
    "cnt = 0\n",
    "for x, y in datenA2.dtypes:\n",
    "    if y != 'string':\n",
    "        cnt += 1\n",
    "print(f'cnt = {cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222a771",
   "metadata": {},
   "source": [
    "# Aufgabe 3: mache den Code lesbarer\n",
    "datenA3 = spark.read.text(\"frankenstein.txt\").select(length(col(\"value\"))).withColumnRenamed(\"length(value)\", \"numChar\")\n",
    "\n",
    "***BEVOR*** Du den Code ausfuehrst: was ist denn nach dem Kommando im Dataframe datenA3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b91031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "datenA3 = spark.read.text(\"frankenstein.txt\").select(length(col(\"value\"))).withColumnRenamed(\"length(value)\", \"numChar\")\n",
    "datenA3.show(2)\n",
    "datenA3b = spark.read.text(\"frankenstein.txt\").select(length(col(\"value\")).alias(\"numChar\"))\n",
    "datenA3b.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6dcde",
   "metadata": {},
   "source": [
    "# Aufgabe 4: im folgenden Code gibt es ein Problem / was ist es, und wie kann man es reparieren?\n",
    "finde erst einmal heraus, was der Code wohl als Ziel hat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datenA4 = spark.createDataFrame([[\"key\", 20_000_000, 10_000_000_000]], [\"key\", \"value1\", \"value2\"])\n",
    "datenA4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ea186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import greatest\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "try:\n",
    "    datenA4M = datenA4.select(greatest(col(\"value1\"), col(\"value2\")).alias(\"maxVal\")).select(\"key\",\"maxVal\")\n",
    "except AnalysisException as err:\n",
    "        print(f\"das war nicht gut: {err}\")\n",
    "datenA4M.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2acae",
   "metadata": {},
   "source": [
    "# Aufgabe 5 filtere einen ganzen Haufen Woerter raus\n",
    "mit Hilfe der Funktion isin filtere die Woerter \"is\", \"not\", \"if\", \"the\" aus dem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ee28dc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          word|\n",
      "+--------------+\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|  frankenstein|\n",
      "|            by|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|        godwin|\n",
      "|       shelley|\n",
      "|          this|\n",
      "|         ebook|\n",
      "|           use|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|          cost|\n",
      "|          with|\n",
      "|        almost|\n",
      "|  restrictions|\n",
      "|    whatsoever|\n",
      "|           you|\n",
      "|           may|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exclDict = [\"is\", \"not\", \"if\", \"the\", \"for\", \"of\", \"no\", \"at\", \"and\"]\n",
    "words_no_dict = words_nonull.where(~col(\"word\").isin(exclDict))\n",
    "words_no_dict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "605324b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aufgabe 5b: versuche die exclusionWords in einem Dataframe darzustellen, und dann diesen im \"isin\" zu nutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e883c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   is|\n",
      "|  not|\n",
      "|   if|\n",
      "|  the|\n",
      "|  for|\n",
      "|   of|\n",
      "|   no|\n",
      "|   at|\n",
      "|  and|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m exclusionWords \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m exclusionWords\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 4\u001b[0m words_no_dict2 \u001b[38;5;241m=\u001b[39m words_nonull\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;241m~\u001b[39m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclusionWords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:597\u001b[0m, in \u001b[0;36mColumn.isin\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    596\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 597\u001b[0m cols \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01melse\u001b[39;00m _create_column_from_literal(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m    598\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    599\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misin\u001b[39m\u001b[38;5;124m\"\u001b[39m)(_to_seq(sc, cols))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:597\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    596\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 597\u001b[0m cols \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_create_column_from_literal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m    598\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    599\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misin\u001b[39m\u001b[38;5;124m\"\u001b[39m)(_to_seq(sc, cols))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:31\u001b[0m, in \u001b[0;36m_create_column_from_literal\u001b[0;34m(literal)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_from_literal\u001b[39m(literal):\n\u001b[1;32m     30\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliteral\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1313\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1313\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m   1280\u001b[0m     temp_args \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1282\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1283\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m   1280\u001b[0m     temp_args \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1282\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1283\u001b[0m     [\u001b[43mget_command_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:298\u001b[0m, in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    296\u001b[0m         command_part \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m interface\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     command_part \u001b[38;5;241m=\u001b[39m REFERENCE_TYPE \u001b[38;5;241m+\u001b[39m \u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_id\u001b[49m()\n\u001b[1;32m    300\u001b[0m command_part \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m command_part\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1659\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1660\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1661\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "# Antwort> geht leider nicht\n",
    "exclusionWords = spark.createDataFrame([\"is\", \"not\", \"if\", \"the\", \"for\", \"of\", \"no\", \"at\", \"and\"], \"string\")\n",
    "exclusionWords.show()\n",
    "words_no_dict2 = words_nonull.where(~col(\"word\").isin(exclusionWords.select(\"value\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb523b1f",
   "metadata": {},
   "source": [
    "# Aufgabe 6: Debugging\n",
    "Finde den/die Fehler im folgenden Code und repariere in so, dass der Code so wie wohl erwartet funktioniert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27248c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "try:\n",
    "    book = spark.read.text(\"frankenstein.txt\")\n",
    "    book = book.printSchema()\n",
    "    lines= book.select(spolit(book.value, \" \").alias(\"lime\"))\n",
    "    words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "    print(err)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5b764b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-----+\n",
      "| word|\n",
      "+-----+\n",
      "|hello|\n",
      "|world|\n",
      "| this|\n",
      "| boat|\n",
      "|  has|\n",
      "|     |\n",
      "|   no|\n",
      "| oars|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Antwort\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.utils import AnalysisException # import wird unten benoetigt\n",
    "\n",
    "try:\n",
    "    book = spark.read.text(\"frankenstein.txt\")\n",
    "    book.printSchema()   # bitte nicht das Ergebnis von printSchema an den Dataframe zuweisen\n",
    "    lines= book.select(split(book.value, \" \").alias(\"line\"))  # Tippfehler: spolit und lime anstatt line\n",
    "    words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:   # import\n",
    "    print(err)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63338f43",
   "metadata": {},
   "source": [
    "# Gruppieren\n",
    "Ziel: jetzt wollen wir zaehlen, wie oft jedes Wort vorkommt\n",
    ".count() alleine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddb07521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77907"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ne, bringt nix, das sind ja nur \"alle Worte\" (Anzahl Reihen)\n",
    "words_nonull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "501f4363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[word: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "# also: erstmal Gruppieren\n",
    "groups = words_nonull.groupby(col(\"word\"))\n",
    "print(groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "18df867f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|hello|    1|\n",
      "| oars|    1|\n",
      "| boat|    1|\n",
      "|   no|    1|\n",
      "|world|    2|\n",
      "| this|    1|\n",
      "|  has|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCounts = groups.count()\n",
    "wordCounts.printSchema()\n",
    "wordCounts.show()\n",
    "# Frage: kommen bei allen die Ergebnisse in der gleichen Reihenfolge heraus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660adfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antwort: sollte eigentlich nicht, da das Processing ueber mehrere Nodes verteilt wird..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60151e",
   "metadata": {},
   "source": [
    "# Aufgabe: finde die Anzahl der Worte per Anzahl Buchstaben (also: wie viele Worte mit 1, 2, 3, ... Buchstaben,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e75ffcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     2|    1|\n",
      "|     3|    2|\n",
      "|     4|    3|\n",
      "|     5|    3|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Antwort\n",
    "from pyspark.sql.functions import length\n",
    "# words_nonull.groupBy(col(\"word\")).count().select(\"count\").alias(\"len\").groupBy(col(\"len\")).count().show()\n",
    "words_nonull.select(length(col(\"word\")).alias(\"length\")).groupBy(\"length\").count().orderBy(col(\"length\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d175b2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|    3|\n",
      "|     4|    3|\n",
      "|     3|    2|\n",
      "|     2|    1|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|    3|\n",
      "|     4|    3|\n",
      "|     3|    2|\n",
      "|     2|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCount = words_nonull.select(length(col(\"word\")).alias(\"length\")).groupBy(\"length\").count()\n",
    "# hier 2 Alternativen des orderBy\n",
    "\n",
    "wordCount.orderBy(\"count\", ascending = False).show()\n",
    "wordCount.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97216291",
   "metadata": {},
   "source": [
    "# Ach uebrigens\n",
    "was faellt Dir auf:\n",
    "\n",
    "regexp_extract, groupBy, groupby, orderBy (probier mal orderby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ca16f",
   "metadata": {},
   "source": [
    "Antwort: die Cases sind all over the place. Ist leider so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0afc268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# was macht das hier?\n",
    "wordCount.write.csv(\"dingdong.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e04b030f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          word|\n",
      "+--------------+\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|  frankenstein|\n",
      "|            by|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|        godwin|\n",
      "|       shelley|\n",
      "|          this|\n",
      "|         ebook|\n",
      "|            is|\n",
      "|           for|\n",
      "|           the|\n",
      "|           use|\n",
      "|            of|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|            at|\n",
      "|            no|\n",
      "|          cost|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Antwort: es legt ein Directory an! \n",
    "# was passiert wenn wir das fuer ein grosses File machen?\n",
    "words_nonull.show()\n",
    "words_nonull.write.csv(\"frankenout2.csv\")\n",
    "# Antwort: wenn ich mehrere Worker habe, dann gibt es (oft) mehrere Files. Diese\n",
    "# kann man mit .coalesce zusammenfuegen\n",
    "words_nonull.coalesce(1).write.csv(\"frankenCoalesce1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce817d",
   "metadata": {},
   "source": [
    "# Aufraeumen\n",
    "import as F\n",
    "brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "results = (\n",
    "    spark.read.text(\"frankenstein.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(F.col(\"word\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    "results.coalesce(1).write.csv(\"./results_single_partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08078a8e",
   "metadata": {},
   "source": [
    "# Abschluss-Aufgabe\n",
    "lade einen Citybike Datensatz, erstelle ein Ranking nach Anzahl der von den jeweiligen Stationen ausgehenden Fahrten\n",
    "und speichere das Ergebnis in einer Datei \"abgehendeFahrten.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
