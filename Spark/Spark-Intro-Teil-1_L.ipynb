{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b35570",
   "metadata": {},
   "source": [
    "# Was ist Spark?\n",
    "https://chartio.com/learn/data-analytics/what-is-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bcdda8",
   "metadata": {},
   "source": [
    "# Ziel: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "Aufgabe: erstelle ein Programm, das die Zeilen in diesem Sourcecode zaehlt, die das Wort \"Spark\" enthalten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52df7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brauchen wir nicht (zumindest bisher;-)\n",
    "\n",
    "# erstelle einen Context mit einer bestimmten Configuration\n",
    "from pyspark import SparkConf, SparkContext\n",
    "config = SparkConf().setAppName(\"MySparkApp\")\n",
    "context = SparkContext(conf=config)\n",
    "# Aufgabe: fuehre diese Zelle 2x aus. Was passiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir wollen DataFrames nutzen, und dafuer brauchen wir eine SparkSession\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"Datenbanken mit Spark\") \\\n",
    "      .getOrCreate() # getOrCreate liefert existierende Session wenn es schon eine gibt\n",
    "\n",
    "# Aufgabe: fuehre diese Zelle 2x aus. Was passiert? Gibt es einen Unterschied zur Aufgabe davor?\n",
    "# versuche, die obere Zelle zu \"reparieren\" damit sie das gleiche Verhalten (das Selbe????) zeigt, wie diese hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# den Inhalt des directories anschauen. Bei mir gibt es ein File \"Spark Intro.ipynb\")\n",
    "import os\n",
    "for subdir, dirs, files in os.walk('./'):\n",
    "    for file in files:\n",
    "      print(file)\n",
    "    \n",
    "# !ls geht natuerlich schneller;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dieses File (das wir hier gerade bearbeiten) einlesen\n",
    "textFile = spark.read.text(\"Spark Intro.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d75b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wie viele Zeilen ...\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.first()   # warum kommt da \"Mist\" raus? eigentlich sollte ja die erste Zeile kommen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b155e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textFile.show(6)  # nur die ersten 6 Zeilen zeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8108eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jetzt filtern wir die Zeilen mit \"Spark\" drin raus\n",
    "linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee48973",
   "metadata": {},
   "source": [
    "# Aufgabe: finde heraus, wie viele Zeilen das Wort \"Spark\" enthalten,\n",
    "wie viele den Inhalt \"spark\" haben, und zeige dann den Inhalt dieses neuen Dataframe an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.filter(textFile.value.contains(\"spark\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50b5c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jetzt ein bisschen SQL\n",
    "from pyspark.sql.functions import *\n",
    "textFile.select(size(split(textFile.value, \"\\s+\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfd7ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wie in SQL koennen wir einer Spalte einen Namen geben: .name(\"name der Spalte\")\n",
    "# Obacht: das ist Teil des SELECT-statements! (Klammern beachten)\n",
    "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ef5ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxWords=textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\")))\n",
    "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd167382",
   "metadata": {},
   "source": [
    "# Aufgabe: extrahiere programmatisch die maximale Anzahl von Worten in einer Zeile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053178c4",
   "metadata": {},
   "source": [
    "Bsp: https://www.geeksforgeeks.org/pyspark-extracting-single-value-from-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba954fb",
   "metadata": {},
   "source": [
    "was macht .head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxWords.head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# was macht \"first\"?\n",
    "maxWords.first()['max(numWords)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da89bd",
   "metadata": {},
   "source": [
    "man muss wirklich den column-Namen nehmen, in diesem Fall 'max(numWords)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9aaecb",
   "metadata": {},
   "source": [
    "# Aufgabe: CSV File einlesen\n",
    "\n",
    "Lies die CSV Datei \"studibier.csv\" aus Moodle in spark ein, (und berechne das Durchschnittsalter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat studibier.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7598f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbier = spark.read.csv('studibier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbier.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23422a",
   "metadata": {},
   "source": [
    "repariere die komischen header...\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd416e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbier = spark.read.option('header', 'true').csv('studibier.csv')\n",
    "df_sbier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42792e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datentypen anzeigen\n",
    "if df_sbier.dtypes[1][1] == 'string':\n",
    "    print(\"it's a string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe\n",
    "df_sbier.describe()\n",
    "#df_sbier.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9b73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbier.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3b52d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbier.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbier = spark.read.option('header', 'true').csv('studibier.csv', inferSchema=True)\n",
    "df_sbier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84665d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbier = spark.read.csv('studibier.csv', header=True, inferSchema=True)\n",
    "df_sbier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f16cc8",
   "metadata": {},
   "source": [
    "# Columns, Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefa424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbier.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d868e28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# zeige nur die verschiedenen Biersorten (A: \"distinct\" herausfinden)\n",
    "df_sbier.select(' Lieblingsbier').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae96b66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Aufgabe: finde das Durchschnittsalter ...\n",
    "# funktioniert avg, oder mean?\n",
    "# nope...\n",
    "df_sbier.select(' Alter').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf03efa",
   "metadata": {},
   "source": [
    "# Aufgabe: und wie loest man das jetzt?\n",
    "mit withColumnRenamed oder Quelldaten fixen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbier.withColumnRenamed(' Alter', 'Alter').agg({\"Alter\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84668508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbier.agg({\" Alter\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc053aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiple columns\n",
    "df_sbier.select([' Lieblingsbier', ' Alter']).show()\n",
    "# was macht distinct?\n",
    "# Antwort: es zeigt einzigartige komplette Zeilen (also alle Spalten muessen identisch sein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213a31b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# was passiert denn hier?\n",
    "from pyspark.sql.functions import lit\n",
    "df_sbier.withColumn('Alter nach 2 Jahren', df_sbier[' Alter']+2).withColumn('Eins', lit(1)).show()\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-add-new-column-to-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d81532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ja wo sind sie denn, die schoenen neuen Spalten?\n",
    "df_sbier.head(5)\n",
    "# Antwort: da wurde halt in memory geaendert, bzw neu angelegt, das Original wird NICHT geaendert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399fe8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbierMitMuell = df_sbier.withColumn('Alter nach 2 Jahren', df_sbier[' Alter']+2).withColumn('Eins', lit(1))\n",
    "df_sbierMitMuell.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe60939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbierMitMuell.drop('Alter nach 2 Jahren').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15921744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbierMitMuell.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c31ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sbierMitMuell.na.drop(how=\"any\",thresh=1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbierMitMuell.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb86721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aufgabe: fuege 3 Zeilen hinzu, die 1, 2, oder 3 NULL Werte enthalten\n",
    "# z.B. https://mungingdata.com/pyspark/none-null/\n",
    "nullWerteDaten = [('John', 'Weizen', 14.5, None, 2),(None, 'NEIPA', None, None, 2),('Jane','Pils',None,None,2)]\n",
    "nwdSchema = df_sbierMitMuell.schema\n",
    "df_bierWithNull = df_sbierMitMuell.union(spark.createDataFrame(data=nullWerteDaten, schema = nwdSchema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bierWithNull.show()\n",
    "df_bierWithNull.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26340ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Aufgabe: na.drop entferne Zeilen die 1, 2 oder 3 NULL enthalten\n",
    "df_bierWithNull.na.drop(how=\"any\",thresh=5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48fa4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aufgabe: na.drop entferne Zeilen die 1, 2 oder 3 NULL enthalten / ***aber nur wenn das Alter fehlt****\n",
    "df_bierWithNull.na.drop(how=\"any\",subset=[' Alter']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511fa31",
   "metadata": {},
   "source": [
    "# Aufgabe: was mache ich denn wenn ich nicht droppen will, sondern ersetzen?\n",
    "Antwort: mit fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabaddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# was passiert hier/was ist unerwartet?\n",
    "df_bierWithNull.na.fill('so nicht').show()\n",
    "# Antwort: fill schaut sich den Datentyp an und fuellt nur das was passt (int&double kann nicht mit Text gefuellt werden\n",
    "\n",
    "# Aufgabe 2: fuelle nur \"Alter nach 2 Jahren\"-Null-Werte ()\n",
    "df_bierWithNull.na.fill(99,'Alter nach 2 Jahren').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63bd6b9",
   "metadata": {},
   "source": [
    "# JOINS \n",
    "https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449fa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49d1be6d",
   "metadata": {},
   "source": [
    "# Citibike Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ef1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_citibikeOct = spark.read.option('header','true').csv('/mnt/macbook/spark/202210-citibike-tripdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_citibikeOct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42838fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_citibikeOct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e4d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_citibikeOct.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
